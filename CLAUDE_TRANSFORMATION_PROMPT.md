# ðŸš€ PROMPT FOR CLAUDE: Transform ContextVault into Universal AI Memory CLI

Copy this entire prompt and give it to Claude in Cursor or any other interface.

---

# TRANSFORMATION TASK: ContextVault â†’ Universal AI Memory CLI

## ðŸ“‹ CURRENT STATE

You have a working ContextVault system at `/Users/aniksahai/Desktop/contextvault` with:

**Working Features:**
- Traditional RAG system with SQLite database (480 entries)
- Ollama proxy on port 11435
- Semantic search with sentence-transformers
- Permission system (16 rules)
- CLI commands via `contextible`
- REST API with FastAPI

**Inactive But Complete Code:**
- Cognitive Workspace: `contextvault/cognitive/workspace.py` (671 lines) - 3-layer buffers, attention manager, forgetting curves
- Graph RAG: `contextvault/storage/graph_db.py` (689 lines) - Neo4j integration, entity extraction
- Document Ingestion: `contextvault/storage/document_ingestion.py` (597 lines) - PDF/DOCX/TXT support
- Feed Command: `contextvault/cli/commands/feed.py` (166 lines)
- Recall Command: `contextvault/cli/commands/recall.py` (197 lines)
- Graph RAG CLI: `contextvault/cli/commands/graph_rag.py` (345 lines)

**Dependencies Already Installed:**
- neo4j (6.0.2), networkx (3.5), spacy (3.8.7)
- sentence-transformers (5.1.1), fastapi (0.104.1), click (8.2.1), rich (14.1.0)

## ðŸŽ¯ TARGET STATE

Transform this into a **Universal AI Memory CLI** that:

1. **Works with ANY local AI model** (Ollama, LMStudio, LocalAI, llamafile)
2. **Uses Cognitive Workspace** with 3-layer memory buffers (8K/64K/256K tokens)
3. **Integrates Mem0** as the universal memory layer
4. **Has knowledge graphs** with NetworkX/Neo4j
5. **Background consolidation** of memories (patterns, deduplication, forgetting)
6. **Session state save/restore** for persistent conversations
7. **Simple CLI**: `ai-memory chat`, `ai-memory feed`, `ai-memory recall`, `ai-memory status`
8. **Interactive chat mode** like ChatGPT
9. **Model selection**: `--model ollama:llama3.1` or `--model lmstudio:mistral`
10. **Qdrant or FAISS** vector store for production

## âœ… ACCEPTANCE CRITERIA

After completion, these commands must work:

```bash
# Setup
ai-memory setup --model ollama:llama3.1

# Add documents
ai-memory feed research_paper.pdf notes.md

# Interactive chat with memory
ai-memory chat --session my_project

# Search memory directly
ai-memory recall "machine learning notes"

# Check memory status
ai-memory status

# Switch models
ai-memory chat --model lmstudio:mistral:latest

# Reset memory (with confirmation)
ai-memory reset
```

## ðŸ”§ SPECIFIC CHANGES REQUIRED

### PHASE 1: ACTIVATE EXISTING CODE (High Priority - 3 hours)

#### 1.1 Integrate Cognitive Workspace

**File:** `scripts/ollama_proxy.py` (or create `scripts/universal_proxy.py`)

**Current (around line 160):**
```python
context_text = "\n\n".join([entry.content for entry in relevant_entries])
modified_prompt = f"Context: {context_text}\n\nUser: {prompt}"
```

**Change to:**
```python
from contextvault.cognitive import cognitive_workspace

formatted_context, stats = cognitive_workspace.load_query_context(
    query=prompt,
    relevant_memories=[{
        "id": e.id,
        "content": e.content,
        "metadata": e.entry_metadata or {},
        "relevance_score": getattr(e, 'relevance_score', 0.5)
    } for e in relevant_entries]
)

modified_prompt = formatted_context
logger.info(f"Workspace stats: {stats}")
```

#### 1.2 Register Missing CLI Commands

**File:** `contextvault/cli/main.py`

**Current imports (line 21-24):**
```python
from contextvault.cli.commands import (
    system, context, permissions, templates,
    test, demo, diagnose, config, setup, mcp, learning
)
```

**Change to:**
```python
from contextvault.cli.commands import (
    system, context, permissions, templates,
    test, demo, diagnose, config, setup, mcp, learning,
    feed, recall, graph_rag, settings  # ADD THESE
)
```

**After line 116, add:**
```python
cli.add_command(feed.feed_command)
cli.add_command(recall.recall_command)
cli.add_command(graph_rag.graph_rag_group)
cli.add_command(settings.settings_group)
```

#### 1.3 Rename CLI to ai-memory

**File:** `pyproject.toml`

**Current (around line 50):**
```toml
[project.scripts]
contextible = "contextvault.cli.main:cli"
```

**Change to:**
```toml
[project.scripts]
ai-memory = "contextvault.cli.main:cli"
contextible = "contextvault.cli.main:cli"  # Keep for backward compatibility
```

**File:** `contextvault/cli/main.py` (line 75-89)

**Update docstring:**
```python
def cli(ctx, no_banner):
    """
    âœ¨ AI Memory - Universal Memory for Local AI Models âœ¨

    Transform any local AI into a persistent, context-aware assistant.
    Works with Ollama, LMStudio, LocalAI, and llamafile.

    Features:
    â€¢ ðŸ§  Persistent memory across sessions
    â€¢ ðŸ” Semantic search with attention management
    â€¢ ðŸ“š 3-layer cognitive workspace (8K/64K/256K)
    â€¢ ðŸ•¸ï¸ Knowledge graphs for relationship tracking
    â€¢ ðŸŽ¨ Beautiful CLI with rich output
    â€¢ ðŸ”’ 100% local and private

    Get started: ai-memory setup
    """
```

#### 1.4 Add Interactive Chat Command

**Create:** `contextvault/cli/commands/chat.py`

```python
"""Interactive chat command for AI Memory."""

import click
from rich.console import Console
from rich.prompt import Prompt
from rich.panel import Panel
from rich.markdown import Markdown
import requests
from typing import Optional

console = Console()

@click.command()
@click.argument('query', required=False)
@click.option('--session', help='Session name for persistent context')
@click.option('--model', default='ollama:llama3.1', help='Model to use (format: provider:model:tag)')
@click.option('--memory-dir', default=None, help='Custom memory directory')
def chat(query: Optional[str], session: Optional[str], model: str, memory_dir: Optional[str]):
    """Chat with AI using persistent memory.
    
    Examples:
        ai-memory chat "What are my research topics?"
        ai-memory chat --session project_x
        ai-memory chat --model lmstudio:mistral:latest
    """
    
    console.print(Panel(
        "[bold cyan]AI Memory Chat[/bold cyan]\n"
        f"Model: {model}\n"
        f"Session: {session or 'default'}\n"
        f"Type 'exit' or 'quit' to end conversation",
        border_style="cyan"
    ))
    
    # Single query mode
    if query:
        response = send_query(query, session, model)
        console.print("\n[bold green]AI:[/bold green]")
        console.print(Markdown(response))
        return
    
    # Interactive mode
    conversation_loop(session, model, memory_dir)


def conversation_loop(session: Optional[str], model: str, memory_dir: Optional[str]):
    """Interactive conversation loop."""
    
    while True:
        try:
            # Get user input
            user_input = Prompt.ask("\n[bold blue]You[/bold blue]")
            
            if user_input.lower() in ['exit', 'quit', 'bye']:
                console.print("\nðŸ‘‹ [yellow]Goodbye! Your memories are saved.[/yellow]")
                break
            
            if not user_input.strip():
                continue
            
            # Show thinking indicator
            with console.status("[bold cyan]Thinking...", spinner="dots"):
                response = send_query(user_input, session, model)
            
            # Display response
            console.print("\n[bold green]AI:[/bold green]")
            console.print(Markdown(response))
            
        except KeyboardInterrupt:
            console.print("\n\nðŸ‘‹ [yellow]Chat interrupted. Your memories are saved.[/yellow]")
            break
        except Exception as e:
            console.print(f"\nâŒ [red]Error: {e}[/red]")


def send_query(query: str, session: Optional[str], model: str) -> str:
    """Send query to AI with memory context."""
    
    try:
        # Parse model string (provider:model:tag)
        parts = model.split(':')
        provider = parts[0]
        model_name = ':'.join(parts[1:]) if len(parts) > 1 else 'llama3.1'
        
        # Get context from memory
        memory_context = retrieve_memory_context(query, session)
        
        # Build prompt with memory
        full_prompt = build_prompt_with_memory(query, memory_context)
        
        # Send to appropriate provider
        if provider == 'ollama':
            response = call_ollama(model_name, full_prompt)
        elif provider == 'lmstudio':
            response = call_lmstudio(model_name, full_prompt)
        elif provider == 'localai':
            response = call_localai(model_name, full_prompt)
        elif provider == 'llamafile':
            response = call_llamafile(model_name, full_prompt)
        else:
            response = f"Unknown provider: {provider}"
        
        # Store interaction in memory
        store_interaction(query, response, session)
        
        return response
        
    except Exception as e:
        return f"Error: {str(e)}"


def retrieve_memory_context(query: str, session: Optional[str]) -> str:
    """Retrieve relevant context from memory."""
    try:
        response = requests.post('http://localhost:11435/api/context/search', json={
            'query': query,
            'limit': 10,
            'session': session
        })
        
        if response.status_code == 200:
            results = response.json()
            entries = results.get('entries', [])
            return "\n".join([e['content'] for e in entries])
        else:
            return ""
    except:
        return ""


def build_prompt_with_memory(query: str, memory_context: str) -> str:
    """Build prompt with memory context."""
    if memory_context:
        return f"""Based on the following context about the user:

{memory_context}

User Question: {query}

Please provide a helpful response using the context above."""
    else:
        return query


def call_ollama(model: str, prompt: str) -> str:
    """Call Ollama API."""
    try:
        response = requests.post('http://localhost:11434/api/generate', json={
            'model': model,
            'prompt': prompt,
            'stream': False
        }, timeout=60)
        
        if response.status_code == 200:
            return response.json().get('response', 'No response')
        else:
            return f"Ollama error: {response.status_code}"
    except Exception as e:
        return f"Ollama connection error: {str(e)}"


def call_lmstudio(model: str, prompt: str) -> str:
    """Call LMStudio API (OpenAI-compatible)."""
    try:
        response = requests.post('http://localhost:1234/v1/chat/completions', json={
            'model': model,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': 0.7
        }, timeout=60)
        
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            return f"LMStudio error: {response.status_code}"
    except Exception as e:
        return f"LMStudio connection error: {str(e)}"


def call_localai(model: str, prompt: str) -> str:
    """Call LocalAI API (OpenAI-compatible)."""
    try:
        response = requests.post('http://localhost:8080/v1/chat/completions', json={
            'model': model,
            'messages': [{'role': 'user', 'content': prompt}]
        }, timeout=60)
        
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            return f"LocalAI error: {response.status_code}"
    except Exception as e:
        return f"LocalAI connection error: {str(e)}"


def call_llamafile(path: str, prompt: str) -> str:
    """Call llamafile directly."""
    import subprocess
    try:
        result = subprocess.run(
            [path, '--prompt', prompt, '--silent'],
            capture_output=True,
            text=True,
            timeout=60
        )
        return result.stdout.strip()
    except Exception as e:
        return f"Llamafile error: {str(e)}"


def store_interaction(query: str, response: str, session: Optional[str]):
    """Store interaction in memory."""
    try:
        requests.post('http://localhost:11435/api/context/add', json={
            'content': f"User: {query}\nAI: {response}",
            'context_type': 'text',
            'tags': ['conversation', session] if session else ['conversation'],
            'metadata': {'session': session, 'type': 'interaction'}
        })
    except:
        pass  # Fail silently if memory storage fails
```

**Register in main.py:**
```python
from contextvault.cli.commands import chat
cli.add_command(chat.chat)
```

### PHASE 2: ADD MEM0 INTEGRATION (Medium Priority - 6 hours)

#### 2.1 Install Mem0

**File:** `requirements.txt`

Add:
```
mem0ai>=0.1.0
qdrant-client>=1.7.0
```

#### 2.2 Create Mem0 Service

**Create:** `contextvault/services/mem0_service.py`

```python
"""Mem0 integration for universal memory management."""

import logging
from typing import List, Dict, Any, Optional
from mem0 import Memory

logger = logging.getLogger(__name__)

class Mem0Service:
    """Wrapper for Mem0 universal memory layer."""
    
    def __init__(self, use_qdrant: bool = False):
        """Initialize Mem0 with appropriate backend.
        
        Args:
            use_qdrant: If True, use Qdrant. Otherwise use in-memory/SQLite.
        """
        
        if use_qdrant:
            config = {
                "vector_store": {
                    "provider": "qdrant",
                    "config": {
                        "host": "localhost",
                        "port": 6333,
                        "collection_name": "ai_memory"
                    }
                }
            }
        else:
            # Use local SQLite + in-memory vectors
            config = {
                "vector_store": {
                    "provider": "chroma",
                    "config": {
                        "path": "./contextvault/storage/chroma"
                    }
                }
            }
        
        try:
            self.memory = Memory.from_config(config)
            self.available = True
            logger.info("Mem0 initialized successfully")
        except Exception as e:
            logger.warning(f"Mem0 initialization failed: {e}")
            self.memory = None
            self.available = False
    
    def add(self, content: str, user_id: str = "default", metadata: Optional[Dict] = None) -> Dict:
        """Add memory to Mem0.
        
        Args:
            content: Memory content
            user_id: User/session identifier
            metadata: Additional metadata
            
        Returns:
            Result dictionary with memory ID
        """
        if not self.available:
            raise RuntimeError("Mem0 is not available")
        
        result = self.memory.add(content, user_id=user_id, metadata=metadata)
        logger.info(f"Added memory to Mem0: {result}")
        return result
    
    def search(self, query: str, user_id: str = "default", limit: int = 10) -> List[Dict[str, Any]]:
        """Search memories in Mem0.
        
        Args:
            query: Search query
            user_id: User/session identifier
            limit: Maximum results
            
        Returns:
            List of matching memories with scores
        """
        if not self.available:
            return []
        
        results = self.memory.search(query, user_id=user_id, limit=limit)
        return results
    
    def get_all(self, user_id: str = "default") -> List[Dict[str, Any]]:
        """Get all memories for a user.
        
        Args:
            user_id: User/session identifier
            
        Returns:
            List of all memories
        """
        if not self.available:
            return []
        
        return self.memory.get_all(user_id=user_id)
    
    def update(self, memory_id: str, content: str, user_id: str = "default") -> Dict:
        """Update existing memory.
        
        Args:
            memory_id: Memory identifier
            content: New content
            user_id: User/session identifier
            
        Returns:
            Updated memory
        """
        if not self.available:
            raise RuntimeError("Mem0 is not available")
        
        return self.memory.update(memory_id, content, user_id=user_id)
    
    def delete(self, memory_id: str, user_id: str = "default") -> bool:
        """Delete a memory.
        
        Args:
            memory_id: Memory identifier
            user_id: User/session identifier
            
        Returns:
            True if deleted successfully
        """
        if not self.available:
            return False
        
        self.memory.delete(memory_id, user_id=user_id)
        return True


# Global instance
try:
    mem0_service = Mem0Service(use_qdrant=False)  # Start with Chroma
except Exception as e:
    logger.warning(f"Failed to create Mem0 service: {e}")
    mem0_service = None
```

#### 2.3 Add Config for Mem0

**File:** `contextvault/config.py`

Add to Settings class:
```python
# Mem0 Configuration
use_mem0: bool = False  # Enable Mem0 instead of VaultService
use_qdrant: bool = False  # Use Qdrant instead of Chroma
qdrant_host: str = "localhost"
qdrant_port: int = 6333
```

### PHASE 3: ADD BACKGROUND CONSOLIDATION (Medium Priority - 10 hours)

**Create:** `contextvault/services/consolidation.py`

```python
"""Background memory consolidation service."""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Set
from collections import Counter
import numpy as np
from sklearn.cluster import DBSCAN

logger = logging.getLogger(__name__)

class MemoryConsolidator:
    """Background service for memory consolidation."""
    
    def __init__(self, memory_service, interval_hours: int = 24):
        """Initialize consolidator.
        
        Args:
            memory_service: Memory service (VaultService or Mem0Service)
            interval_hours: Hours between consolidation runs
        """
        self.memory_service = memory_service
        self.interval_hours = interval_hours
        self.running = False
    
    async def start(self):
        """Start background consolidation loop."""
        self.running = True
        logger.info(f"Starting memory consolidation (every {self.interval_hours}h)")
        
        while self.running:
            try:
                await asyncio.sleep(self.interval_hours * 3600)
                await self.consolidate()
            except Exception as e:
                logger.error(f"Consolidation error: {e}")
    
    def stop(self):
        """Stop consolidation."""
        self.running = False
        logger.info("Stopped memory consolidation")
    
    async def consolidate(self):
        """Run consolidation process."""
        logger.info("Starting memory consolidation...")
        
        # 1. Detect and merge similar memories
        merged_count = await self.merge_similar_memories()
        logger.info(f"Merged {merged_count} similar memories")
        
        # 2. Apply forgetting curves
        forgotten_count = await self.apply_forgetting_curves()
        logger.info(f"Applied forgetting curve to {forgotten_count} memories")
        
        # 3. Detect patterns
        patterns = await self.detect_patterns()
        logger.info(f"Detected {len(patterns)} memory patterns")
        
        # 4. Update knowledge graph
        await self.update_knowledge_graph()
        
        logger.info("Memory consolidation complete")
    
    async def merge_similar_memories(self) -> int:
        """Merge very similar memories."""
        # Get all memories
        # Compute embeddings
        # Find duplicates (cosine similarity > 0.95)
        # Merge them
        return 0  # Placeholder
    
    async def apply_forgetting_curves(self) -> int:
        """Apply Ebbinghaus forgetting curve."""
        # For each memory:
        #   Calculate retention = e^(-t/S)
        #   If retention < threshold: reduce weight or archive
        return 0  # Placeholder
    
    async def detect_patterns(self) -> List[Dict]:
        """Detect frequently co-occurring concepts."""
        # Use clustering on embeddings
        # Find common patterns
        return []  # Placeholder
    
    async def update_knowledge_graph(self):
        """Update knowledge graph with new connections."""
        # Extract entities from recent memories
        # Add to graph
        pass


# Global instance (to be started by main app)
consolidator = None

def start_consolidation(memory_service, interval_hours: int = 24):
    """Start background consolidation."""
    global consolidator
    consolidator = MemoryConsolidator(memory_service, interval_hours)
    asyncio.create_task(consolidator.start())

def stop_consolidation():
    """Stop background consolidation."""
    global consolidator
    if consolidator:
        consolidator.stop()
```

### PHASE 4: ENHANCE SESSION MANAGEMENT (Low Priority - 3 hours)

**File:** `contextvault/services/session_manager.py` (create new)

```python
"""Enhanced session state management."""

import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional
from sqlalchemy.orm import Session

from ..models.sessions import Session as SessionModel
from ..cognitive.workspace import CognitiveWorkspace

logger = logging.getLogger(__name__)

class SessionManager:
    """Manage session state save/restore."""
    
    def __init__(self, workspace: CognitiveWorkspace, db_session: Session):
        self.workspace = workspace
        self.db_session = db_session
    
    def save_session(self, session_id: str, metadata: Optional[Dict] = None) -> bool:
        """Save complete workspace state.
        
        Args:
            session_id: Session identifier
            metadata: Additional session metadata
            
        Returns:
            True if saved successfully
        """
        try:
            state = {
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "scratchpad": [self._serialize_item(item) for item in self.workspace.scratchpad.items],
                "task_buffer": [self._serialize_item(item) for item in self.workspace.task_buffer.items],
                "episodic_cache": [self._serialize_item(item) for item in self.workspace.episodic_cache.items],
                "attention_history": [
                    {
                        "timestamp": h["timestamp"].isoformat(),
                        "query": h["query"],
                        "item_count": h["item_count"],
                        "mean_weight": h["mean_weight"]
                    }
                    for h in self.workspace.attention_manager.attention_history
                ],
                "metadata": metadata or {}
            }
            
            # Save to database
            session = self.db_session.query(SessionModel).filter_by(id=session_id).first()
            if session:
                session.session_state = json.dumps(state)
                session.updated_at = datetime.now()
            else:
                session = SessionModel(
                    id=session_id,
                    model_id="unknown",
                    session_state=json.dumps(state),
                    created_at=datetime.now()
                )
                self.db_session.add(session)
            
            self.db_session.commit()
            logger.info(f"Saved session state: {session_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save session: {e}")
            self.db_session.rollback()
            return False
    
    def restore_session(self, session_id: str) -> bool:
        """Restore workspace from saved state.
        
        Args:
            session_id: Session identifier
            
        Returns:
            True if restored successfully
        """
        try:
            session = self.db_session.query(SessionModel).filter_by(id=session_id).first()
            if not session or not session.session_state:
                logger.warning(f"No saved state for session: {session_id}")
                return False
            
            state = json.loads(session.session_state)
            
            # Restore buffers
            self.workspace.scratchpad.items = [
                self._deserialize_item(item) for item in state["scratchpad"]
            ]
            self.workspace.task_buffer.items = [
                self._deserialize_item(item) for item in state["task_buffer"]
            ]
            self.workspace.episodic_cache.items = [
                self._deserialize_item(item) for item in state["episodic_cache"]
            ]
            
            # Recalculate token counts
            self.workspace.scratchpad.current_tokens = sum(
                item.tokens for item in self.workspace.scratchpad.items
            )
            self.workspace.task_buffer.current_tokens = sum(
                item.tokens for item in self.workspace.task_buffer.items
            )
            self.workspace.episodic_cache.current_tokens = sum(
                item.tokens for item in self.workspace.episodic_cache.items
            )
            
            logger.info(f"Restored session state: {session_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to restore session: {e}")
            return False
    
    def _serialize_item(self, item) -> Dict:
        """Serialize a memory item."""
        return {
            "id": item.id,
            "content": item.content,
            "metadata": item.metadata,
            "tokens": item.tokens,
            "relevance_score": item.relevance_score,
            "attention_weight": item.attention_weight,
            "access_count": item.access_count,
            "last_accessed": item.last_accessed.isoformat(),
            "created_at": item.created_at.isoformat()
        }
    
    def _deserialize_item(self, data: Dict):
        """Deserialize a memory item."""
        from ..cognitive.workspace import MemoryItem
        from datetime import datetime
        
        return MemoryItem(
            id=data["id"],
            content=data["content"],
            metadata=data["metadata"],
            tokens=data["tokens"],
            relevance_score=data["relevance_score"],
            attention_weight=data["attention_weight"],
            access_count=data["access_count"],
            last_accessed=datetime.fromisoformat(data["last_accessed"]),
            created_at=datetime.fromisoformat(data["created_at"])
        )
```

## ðŸš€ EXECUTION PLAN

### Step 1: Activate Existing Code (Priority 1 - Do First)
1. Integrate Cognitive Workspace in proxy
2. Register feed, recall, graph_rag commands in main.py
3. Rename CLI to ai-memory in pyproject.toml
4. Create chat.py command
5. Test all commands work

### Step 2: Install and Integrate Dependencies
```bash
pip install mem0ai qdrant-client
```

### Step 3: Create New Services
1. Create mem0_service.py
2. Create consolidation.py
3. Create session_manager.py (enhanced)
4. Update config.py with new settings

### Step 4: Update Main Entry Point

**File:** `contextvault/main.py`

Add startup for background services:
```python
from contextvault.services.consolidation import start_consolidation
from contextvault.services.mem0_service import mem0_service

# On startup
if settings.use_mem0 and mem0_service:
    start_consolidation(mem0_service, interval_hours=24)
```

### Step 5: Reinstall and Test
```bash
cd /Users/aniksahai/Desktop/contextvault
pip install -e .
ai-memory --help
ai-memory chat
ai-memory feed test.txt
ai-memory recall "test"
ai-memory status
```

## âœ… VERIFICATION CHECKLIST

After implementation, verify:

- [ ] `ai-memory --help` shows new commands
- [ ] `ai-memory chat` starts interactive mode
- [ ] `ai-memory feed document.pdf` ingests documents
- [ ] `ai-memory recall "query"` searches memory
- [ ] `ai-memory status` shows memory statistics
- [ ] Cognitive Workspace is active (check logs for "Workspace stats")
- [ ] Session state persists across restarts
- [ ] Can switch models with `--model` flag
- [ ] Ollama integration still works
- [ ] All existing tests still pass

## ðŸ“‹ ADDITIONAL NOTES

1. **Backward Compatibility**: Keep `contextible` command working for existing users
2. **Graceful Degradation**: If Mem0/Qdrant not available, fall back to existing SQLite
3. **Error Handling**: Add try/except around all new integrations
4. **Logging**: Add comprehensive logging for debugging
5. **Documentation**: Update README.md with new commands
6. **Tests**: Add tests for new features

## ðŸŽ¯ SUCCESS CRITERIA

The transformation is complete when:

1. User can run `ai-memory chat` and have interactive conversations with memory
2. User can switch between Ollama/LMStudio/LocalAI with `--model` flag  
3. Cognitive Workspace is active and managing memory in 3 layers
4. Background consolidation is running (check logs)
5. Sessions persist and restore across restarts
6. All original ContextVault features still work
7. New documentation is in place

---

## âš¡ QUICK START (For Claude)

1. Read current code structure
2. Follow Phase 1 changes exactly (activate existing code)
3. Test that basic functionality works
4. Add Phase 2 changes (Mem0)
5. Add Phase 3 and 4 incrementally
6. Test after each phase
7. Update documentation
8. Create comprehensive test suite

Focus on making SMALL, TESTABLE changes. Don't break existing functionality.

---

**REMEMBER:** 60% of the code is already written. Your job is to wire it up and add the missing 40%.

